listings <- read.csv("listings.csv")
# load libraries
library(dplyr)
library(readxl)
library(janitor)
library(ggplot2)
library(skimr)
library(stringr)
listings <- read.csv("listings.csv")
View(listings)
listings <- read.csv("listings.csv")
listings <- read.csv("neighbourhoods.csv")
listings <- read.csv("listings.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
View(neighbourhoods)
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
View(listings)
install.packages("pdftools")
lobster_by_county_url <- "https://www.maine.gov/dmr/commercial-fishing/landings/documents/lobster.county.pdf"
source('~/Downloads/lobstah.R')
library(stringi)
library(pdftools)
library(tidyverse)
library(scales)
# from https://rud.is/b/2018/03/02/comparing-2017-maine-lobster-landings-to-historical-landings/
lobster_by_county_url <- "https://www.maine.gov/dmr/commercial-fishing/landings/documents/lobster.county.pdf"
lobster_by_county_fil <- basename(lobster_by_county_url)
if (!file.exists(lobster_by_county_fil)) download.file(lobster_by_county_url, lobster_by_county_fil)
# read in the PDF
lobster_by_county_pgs <- pdftools::pdf_text(lobster_by_county_fil)
lobster_by_county_df <- lobster_by_county_pgs %>%
map( stri_split_lines) %>% # split each page into lines
flatten() %>%
flatten_chr() %>%
keep(stri_detect_fixed, "$") %>% # keep only lines with "$" in them
stri_trim_both() %>% # clean up white space
stri_split_regex("\ +", simplify = TRUE) %>% # get the columns
as_data_frame() %>%
mutate_at(c("V3", "V4"), parse_number) %>% # turn the formatted text into numbers
set_names(c("year", "county", "pounds", "value")) %>% # better column names
filter(county != "TOTAL") %>%
mutate(year = as.Date(sprintf("%s-01-01", year))) %>% # I like years to be years for plotting
mutate(county = stri_trans_totitle(county))
arrange(lobster_by_county_df, year) %>%
mutate(value = value / 1000000, pounds = pounds / 1000000) %>% # easier on the eyes
group_by(year) %>%
summarise(pounds = sum(pounds), value = sum(value)) %>%
mutate(year_lab = lubridate::year(year)) %>%
mutate(highlight = ifelse(year_lab == 2017, "2017", "Other")) %>% # so we can highlight 2017
ggplot(aes(pounds, value)) +
geom_path() +
geom_label(
aes(label = year_lab, color = highlight, size = highlight),
show.legend = FALSE
) +
scale_y_continuous(label=comma,limits = c(0, 600))+
scale_x_continuous(label=comma,limits = c(0, 150))+
scale_color_manual(values = c("2017" = "#742111", "Other" = "#2b2b2b")) +
scale_size_manual(values = c("2017" = 6, "Other" = 4)) +
labs(
x ="Pounds (millions)",
y = "$ USD (millions)",
title = "Historical Maine Fisheries Landings Data — Lobster (1964-2017)",
subtitle = "All counties combined; Not adjusted for inflation",
caption = "The 2002 & 2003 landings may possibly reflect the increased effort by DMR to collect voluntary landings from some lobster dealers;\nLobster reporting became mandatory in 2004 for all Maine dealers buying directly from harvesters.\nSource: <https://www.maine.gov/dmr/commercial-fishing/landings/historical-data.html>"
)
arrange(lobster_by_county_df, year) %>%
mutate(value = value / 1000000, pounds = pounds / 1000000) %>% # easier on the eyes
group_by(year) %>%
summarise(pounds = sum(pounds), value = sum(value)) %>%
mutate(year_lab = lubridate::year(year)) %>%
ggplot(aes(pounds, value)) +
geom_path() +
geom_label(
aes(label = year_lab),
show.legend = FALSE
)
lobster_by_county_df %>%
arrange( year) %>%
mutate(value = value / 1000000, pounds = pounds / 1000000) %>%
ggplot(aes(x=pounds, y=value)) +
geom_path()+
labs(
x = 'Millions of Pounds',
y = 'Millions of $'
) +
facet_wrap(~ county)
lobster_by_county_df %>%
arrange( year) %>%
mutate(value = value / 1000000, pounds = pounds / 1000000 , price = value/pounds) %>%
ggplot(aes(x=year, y=price, colour = county)) +
geom_line()+
labs(
x = 'Year',
y = '$/Pound'
)
library(tidyverse)
library(tidytext) # new, pls install
library(widyr) # new, pls install
library(igraph) # new, pls install
library(ggraph)
library(ggthemes) # new, pls install
# See https://deanmarchiori.github.io/2018-02-21-simpsons/ for all details
script <- read_csv('./the-simpsons-by-the-data/simpsons_script_lines.csv')
characters <- read_csv('./the-simpsons-by-the-data/simpsons_characters.csv')
episodes <- read_csv('./the-simpsons-by-the-data/simpsons_episodes.csv')
locations <- read_csv('./the-simpsons-by-the-data/simpsons_locations.csv')
moments <- script %>%
left_join(locations, by = c('location_id' = 'id')) %>%
left_join(characters, by = c('character_id' = 'id')) %>%
left_join(episodes, by = c('episode_id' = 'id')) %>%
select(id,
season_number = season,
episode_in_season = number_in_season,
episode_in_series = number_in_series,
episode_title = title,
line_number = number,
timestamp_in_ms,
words = normalized_text,
word_count,
location = normalized_name.x,
character = normalized_name.y,
gender,
original_air_date) %>%
mutate(word_count = as.integer(word_count)) %>%
arrange(id)
glimpse(moments)
tidy_lines <- moments %>%
select(season_number,
episode_in_series,
line_number,
character,
line = words) %>%
filter(complete.cases(.)) %>%
unnest_tokens(word, line)
tidy_lines
top10 <- tidy_lines %>%
select(character, word) %>%
count(character, sort = TRUE) %>%
top_n(10, n)
top10
tidy_lines_top10 <- tidy_lines %>%
count(character, word, sort = TRUE) %>%
ungroup() %>%
semi_join(top10, by = "character") %>%
anti_join(stop_words)
tfidf <- tidy_lines_top10 %>%
bind_tf_idf(word, character, n)
tfidf %>%
group_by(character) %>%
top_n(9, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = character)) +
geom_col(show.legend = FALSE) +
facet_wrap(~character, ncol = 2, scales = "free") +
coord_flip() +
scale_fill_manual(values = c(
"bart simpson" = "#f04830", "c montgomery burns" = "#007878", "grampa simpson" = "#d89090",
"homer simpson" = "#78d8ff", "krusty the clown" = "#009078", "lisa simpson" = "#ffd818",
"marge simpson" = "#d8f0a8", "moe szyslak" = "#787878", "ned flanders" = "#906030","seymour skinner" = "#606078")) +
theme_minimal() +
labs(x = NULL,
y = "tf-idf",
title = "What are the key terms used by Simpsons characters?",
subtitle = "Top 10 characters from 600 episodes 1989 - 2016")
tidy_eps <- tidy_lines %>%
count(character, episode_in_series, word, sort = TRUE) %>%
ungroup() %>%
semi_join(top10, by = "character") %>%
anti_join(stop_words)
simpson_sentiment <- tidy_eps %>%
inner_join(get_sentiments("afinn")) %>%
group_by(character, episode_in_series) %>%
summarise(sentiment = sum(score))
ggplot(simpson_sentiment, aes(episode_in_series, sentiment, colour = character)) +
geom_bar(stat= "identity",show.legend = FALSE) +
facet_wrap(~ character) +
scale_colour_manual(values = c(
"bart simpson" = "#f04830", "c montgomery burns" = "#007878", "grampa simpson" = "#d89090",
"homer simpson" = "#78d8ff", "krusty the clown" = "#009078", "lisa simpson" = "#ffd818",
"marge simpson" = "#d8f0a8", "moe syzslak" = "#787878", "ned flanders" = "#906030","seymour skinner" = "#606078")) +
theme_minimal() +
labs(title = 'How does sentiment change in Simpons characters over time?',
subtitle = "Top 10 characters from 600 episodes 1989 - 2016",
x = 'Episode',
y = 'Net Sentiment')
interactions <- moments %>%
group_by(character) %>%
mutate(word_count_total = sum(as.numeric(word_count), na.rm = TRUE)) %>%
select(episode_in_season, location, word_count_total, char = character) %>%
ungroup() %>%
filter(complete.cases(.)) %>%
unique() %>%
bind_cols(scene = group_indices(., episode_in_season, location))
clevel <- 0.25
correlations <- interactions %>%
group_by(char) %>%
filter(n() >= 10) %>%
pairwise_cor(char, scene, sort = TRUE) %>%
filter(correlation > clevel)
verticies <- correlations %>%
gather(col, character) %>%
inner_join(interactions, by = c('character' = 'char')) %>%
select(character, word_count_total) %>%
unique()
correlations %>%
graph_from_data_frame(vertices = verticies) %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation),
edge_colour = "#78d8ff",
show.legend = FALSE) +
geom_node_point(aes(size = (word_count_total)),
color = "#ffd818",
show.legend = FALSE) +
geom_node_text(aes(label = name),
repel = TRUE,
size = 3,
segment.alpha = 0.5,
colour = "white") +
labs(title = "The Simpsons Character Network",
subtitle = "Highly connected characters from 1989 - 2016",
caption = "@deanmarchiori") +
theme_void() +
theme(plot.background = element_rect(fill = 'black'),
plot.title = element_text(colour = "white"),
plot.subtitle  = element_text(colour = "white"),
plot.caption  = element_text(colour = "white"))
install.packages(c("ggraph", "ggthemes", "igraph", "tidytext", "widyr"))
library(tidyverse)
library(tidytext) # new, pls install
library(widyr) # new, pls install
library(igraph) # new, pls install
library(ggraph)
library(ggthemes) # new, pls install
# See https://deanmarchiori.github.io/2018-02-21-simpsons/ for all details
script <- read_csv('./the-simpsons-by-the-data/simpsons_script_lines.csv')
characters <- read_csv('./the-simpsons-by-the-data/simpsons_characters.csv')
episodes <- read_csv('./the-simpsons-by-the-data/simpsons_episodes.csv')
locations <- read_csv('./the-simpsons-by-the-data/simpsons_locations.csv')
moments <- script %>%
left_join(locations, by = c('location_id' = 'id')) %>%
left_join(characters, by = c('character_id' = 'id')) %>%
left_join(episodes, by = c('episode_id' = 'id')) %>%
select(id,
season_number = season,
episode_in_season = number_in_season,
episode_in_series = number_in_series,
episode_title = title,
line_number = number,
timestamp_in_ms,
words = normalized_text,
word_count,
location = normalized_name.x,
character = normalized_name.y,
gender,
original_air_date) %>%
mutate(word_count = as.integer(word_count)) %>%
arrange(id)
glimpse(moments)
tidy_lines <- moments %>%
select(season_number,
episode_in_series,
line_number,
character,
line = words) %>%
filter(complete.cases(.)) %>%
unnest_tokens(word, line)
tidy_lines
top10 <- tidy_lines %>%
select(character, word) %>%
count(character, sort = TRUE) %>%
top_n(10, n)
top10
tidy_lines_top10 <- tidy_lines %>%
count(character, word, sort = TRUE) %>%
ungroup() %>%
semi_join(top10, by = "character") %>%
anti_join(stop_words)
tfidf <- tidy_lines_top10 %>%
bind_tf_idf(word, character, n)
tfidf %>%
group_by(character) %>%
top_n(9, tf_idf) %>%
ungroup() %>%
mutate(word = reorder(word, tf_idf)) %>%
ggplot(aes(word, tf_idf, fill = character)) +
geom_col(show.legend = FALSE) +
facet_wrap(~character, ncol = 2, scales = "free") +
coord_flip() +
scale_fill_manual(values = c(
"bart simpson" = "#f04830", "c montgomery burns" = "#007878", "grampa simpson" = "#d89090",
"homer simpson" = "#78d8ff", "krusty the clown" = "#009078", "lisa simpson" = "#ffd818",
"marge simpson" = "#d8f0a8", "moe szyslak" = "#787878", "ned flanders" = "#906030","seymour skinner" = "#606078")) +
theme_minimal() +
labs(x = NULL,
y = "tf-idf",
title = "What are the key terms used by Simpsons characters?",
subtitle = "Top 10 characters from 600 episodes 1989 - 2016")
tidy_eps <- tidy_lines %>%
count(character, episode_in_series, word, sort = TRUE) %>%
ungroup() %>%
semi_join(top10, by = "character") %>%
anti_join(stop_words)
simpson_sentiment <- tidy_eps %>%
inner_join(get_sentiments("afinn")) %>%
group_by(character, episode_in_series) %>%
summarise(sentiment = sum(score))
ggplot(simpson_sentiment, aes(episode_in_series, sentiment, colour = character)) +
geom_bar(stat= "identity",show.legend = FALSE) +
facet_wrap(~ character) +
scale_colour_manual(values = c(
"bart simpson" = "#f04830", "c montgomery burns" = "#007878", "grampa simpson" = "#d89090",
"homer simpson" = "#78d8ff", "krusty the clown" = "#009078", "lisa simpson" = "#ffd818",
"marge simpson" = "#d8f0a8", "moe syzslak" = "#787878", "ned flanders" = "#906030","seymour skinner" = "#606078")) +
theme_minimal() +
labs(title = 'How does sentiment change in Simpons characters over time?',
subtitle = "Top 10 characters from 600 episodes 1989 - 2016",
x = 'Episode',
y = 'Net Sentiment')
interactions <- moments %>%
group_by(character) %>%
mutate(word_count_total = sum(as.numeric(word_count), na.rm = TRUE)) %>%
select(episode_in_season, location, word_count_total, char = character) %>%
ungroup() %>%
filter(complete.cases(.)) %>%
unique() %>%
bind_cols(scene = group_indices(., episode_in_season, location))
clevel <- 0.25
correlations <- interactions %>%
group_by(char) %>%
filter(n() >= 10) %>%
pairwise_cor(char, scene, sort = TRUE) %>%
filter(correlation > clevel)
verticies <- correlations %>%
gather(col, character) %>%
inner_join(interactions, by = c('character' = 'char')) %>%
select(character, word_count_total) %>%
unique()
correlations %>%
graph_from_data_frame(vertices = verticies) %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = correlation),
edge_colour = "#78d8ff",
show.legend = FALSE) +
geom_node_point(aes(size = (word_count_total)),
color = "#ffd818",
show.legend = FALSE) +
geom_node_text(aes(label = name),
repel = TRUE,
size = 3,
segment.alpha = 0.5,
colour = "white") +
labs(title = "The Simpsons Character Network",
subtitle = "Highly connected characters from 1989 - 2016",
caption = "@deanmarchiori") +
theme_void() +
theme(plot.background = element_rect(fill = 'black'),
plot.title = element_text(colour = "white"),
plot.subtitle  = element_text(colour = "white"),
plot.caption  = element_text(colour = "white"))
install.packages(c("maps", "mapdata"))
# load libraries
library(dplyr)
library(readxl)
library(janitor)
library(ggplot2)
library(skimr)
library(stringr)
library(maps)
library(mapsdata)
# load libraries
library(dplyr)
library(readxl)
library(janitor)
library(ggplot2)
library(skimr)
library(stringr)
library(maps)
library(mapdata)
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
canada <- map_data("canada")
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
canada <- map_data("usa")
View(canada)
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
canada <- map_data("canada")
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
canada <- map_data("Canada")
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
map("worldHires","Canada”, xlim=c(-141,-53), ylim=c(40,85), col="gray90”, fill=TRUE)
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
map("worldHires","Canada", xlim=c(-141,-53), ylim=c(40,85), col="gray90", fill=TRUE)
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
usa <- map_data("usa") # we already did this, but we can do it again
ggplot() + geom_polygon(data = usa, aes(x=long, y = lat, group = group)) +
coord_fixed(1.3)
View(usa)
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
dat <- map_data("world", "canada")
ggplot() + geom_polygon(data = dat, aes(x=long, y = lat, group = group)) +
coord_fixed(1.3)
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
dat <- map_data("world", "canada")
ggplot() + geom_polygon(data = dat, aes(x=long, y = lat, group = group))
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
dat <- map_data("world", "canada")
datt <- map_data("world", "canada","province")
listings <- read.csv("listings-summarized.csv")
neighbourhoods <- read.csv("neighbourhoods.csv")
dat <- map_data("world", "canada")
ggplot() + geom_polygon(data = dat, aes(x=long, y = lat, group = group)) +
coord_fixed(1.3)
install.packages("plotly")
install.packages("plotly")
install.packages("plotly")
library(plotly)
install.packages("plotly")
install.packages("plotly")
install.packages("plotly")
